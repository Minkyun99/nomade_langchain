{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of August 2021, there are eight recognized planets in our solar system. They are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, it is important to note that in 2006, the International Astronomical Union (IAU) redefined the definition of a planet, leading to the reclassification of Pluto as a \"dwarf planet.\"'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# a = llm.predict(\"How many planets are there?\") // text-davinci-003 2024.01.04 out of service -> 실행 안 됨\n",
    "b = chat.predict(\"How many planets are there?\") # In service gpt-3.5-turbo\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1 # temperature가 높을수록 창의력이 떨어짐, 답변의 활용도가 낮음\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Il mio nome è Paolo. La distanza tra il Messico e la Thailandia è di circa 16.000 chilometri.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\"),\n",
    "    AIMessage(content = \"Ciao, mi chiam Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and Thailand. Also, what is your name?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 16,000 kilometers (9,942 miles).'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "## chatPromptTemplate : message로부터 template을 만듬\n",
    "## PromptTemplate : String을 이용해서 template을 만듬\n",
    "## template에는 검증계층(validation layer)이 있음 -> 사용자가 볼 수 없는 곳에서 template이 질문에 대한 변수를 정의함.\n",
    "\n",
    "template = PromptTemplate.from_template(\"what is the distance between {country_a} and {country_b}?\")\n",
    "\n",
    "prompt = template.format(country_a = 'Mexico', country_b=\"Thailand\")\n",
    "## template.format은 질문을 정리해서 보여줌\n",
    "\n",
    "\n",
    "print(type(prompt))\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FewShoutPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the capital of France'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: langchain modules study\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate # 질문을 받고 답변을 할 수 있는 PROMPT를 제작하는 모듈\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate # AI가 답변하는 형식을 학습시킬 수 있는 모듈 -> 대화 방식을 데이터 베이스에 저장하여 가지고 오면 편함\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "t = PromptTemplate(\n",
    "    template=\"what is the capital of {country}\",\n",
    "    input_variables=['country'],\n",
    ")\n",
    "\n",
    "t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI\n",
      "    I know this:\n",
      "    Capital: Ankara\n",
      "    Language: Turkish\n",
      "    Food: Kebab and Baklava\n",
      "    Currency: Turkish Lira"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI\\n    I know this:\\n    Capital: Ankara\\n    Language: Turkish\\n    Food: Kebab and Baklava\\n    Currency: Turkish Lira')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "{\n",
    "    \"question\": \"what do you know about Italy?\",\n",
    "    \"answer\": \"\"\"\n",
    "    Here is what I know:\n",
    "    Capital : Rome\n",
    "    Language: Italian\n",
    "    Good: Pizza and Pasta\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"what do you know about Greece?\",\n",
    "    \"answer\": \"\"\"\n",
    "    I know this:\n",
    "    Capital: Athens\n",
    "    Language: Greek\n",
    "    Good:Souvlaki and Feta Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"what do you kow about France?\",\n",
    "    \"answer\": \"\"\"I know this:\n",
    "    Capital : Paris\n",
    "    Language: French\n",
    "    Food: Wine and Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "}\n",
    "]\n",
    "# chat.predict(\"what do you know about France?\")\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples = examples,\n",
    "    suffix=\"Human: What do you know about{country}?\",\n",
    "    input_variables=['country']\n",
    ")\n",
    "                                              \n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\" : \"Turkey\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "{\n",
    "    \"question\": \"Italy?\",\n",
    "    \"answer\": \"\"\"\n",
    "    Here is what I know:\n",
    "    Capital : Rome\n",
    "    Language: Italian\n",
    "    Good: Pizza and Pasta\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"Greece?\",\n",
    "    \"answer\": \"\"\"\n",
    "    I know this:\n",
    "    Capital: Athens\n",
    "    Language: Greek\n",
    "    Good:Souvlaki and Feta Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"France?\",\n",
    "    \"answer\": \"\"\"I know this:\n",
    "    Capital : Paris\n",
    "    Language: French\n",
    "    Food: Wine and Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"what do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "final_prompt.format(country = \"Japan\")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"country\" : \"Turkey\"}\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "## 결제????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Italy?\\nAI\\n    Here is what I know:\\n    Capital : Rome\\n    Language: Italian\\n    Good: Pizza and Pasta\\n    Currency: Euro\\n\\nHuman: Greece?\\nAI\\n    I know this:\\n    Capital: Athens\\n    Language: Greek\\n    Good:Souvlaki and Feta Cheese\\n    Currency: Euro\\n\\nHuman: France?\\nAII know this:\\n    Capital : Paris\\n    Language: French\\n    Food: Wine and Cheese\\n    Currency: Euro\\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "# 예제들을 형식화함, 예제의 양이 얼마나 되는지 확인할 수 있음\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "{\n",
    "    \"question\": \"Italy?\",\n",
    "    \"answer\": \"\"\"\n",
    "    Here is what I know:\n",
    "    Capital : Rome\n",
    "    Language: Italian\n",
    "    Good: Pizza and Pasta\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"Greece?\",\n",
    "    \"answer\": \"\"\"\n",
    "    I know this:\n",
    "    Capital: Athens\n",
    "    Language: Greek\n",
    "    Good:Souvlaki and Feta Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"France?\",\n",
    "    \"answer\": \"\"\"I know this:\n",
    "    Capital : Paris\n",
    "    Language: French\n",
    "    Food: Wine and Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI{answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt = example_prompt,\n",
    "    max_length = 300,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector = example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=['country']\n",
    ")\n",
    "\n",
    "prompt.format(country = \"Brazil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "# 예제들을 형식화함, 예제의 양이 얼마나 되는지 확인할 수 있음\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "{\n",
    "    \"question\": \"Italy?\",\n",
    "    \"answer\": \"\"\"\n",
    "    Here is what I know:\n",
    "    Capital : Rome\n",
    "    Language: Italian\n",
    "    Good: Pizza and Pasta\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"Greece?\",\n",
    "    \"answer\": \"\"\"\n",
    "    I know this:\n",
    "    Capital: Athens\n",
    "    Language: Greek\n",
    "    Good:Souvlaki and Feta Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "},\n",
    "{\n",
    "    \"question\": \"France?\",\n",
    "    \"answer\": \"\"\"I know this:\n",
    "    Capital : Paris\n",
    "    Language: French\n",
    "    Food: Wine and Cheese\n",
    "    Currency: Euro\"\"\"\n",
    "}\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector = example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=['country']\n",
    ")\n",
    "\n",
    "prompt.format(country = \"Brazil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization and Composition\n",
    "#### 여러 개의 prompt를 만들어서 병합시키는 작업이 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the capital of korea'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "# prompt 파일을 json 또는 yaml 으로 만들어서 import 시킴\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "prompt.format(country = \"korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()\n",
    "    ]\n",
    ")\n",
    "intro = PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
